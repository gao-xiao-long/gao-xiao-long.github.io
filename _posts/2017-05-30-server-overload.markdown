---
layout: post
title: （本文现在为草稿阶段勿看~~）公共代理服务如何做好过载保护
date: 2017-05-30
author: "gao-xiao-long"
catalog: true
tags:
    - 基础技术
---

如下图所示，假设有一个公共代理服务(proxy), 负责将上游请求按照一定规则转发给下游，并将下游返回结果再返回给上游，如何才能使proxy更好的自我保护及保护后端服务，防止出现过载，甚至出现雪崩呢？
![图](/img/in-post/proxy.png)

#### 过载介绍
**什么是过载：**  
在服务器开发中，过载指的是外部请求量已经超过了系统的最大处理能力。比如，假设某系统每秒最多处理100条请求，但是它每秒收到的请求有200条，这时，我们就说系统**已经过载**。

**过载时表现：**  
过载时，**每个请求都需要比以往更长的时间才能得到响应**，如果系统在过载的时候没有做到相应保护，导致历史积累的超时请求达到一定规模，像雪球一样形成恶性循环，会导致系统处理的每个请求都因为超时而无效，**系统对外呈现的服务能力为0，且这种情况下不能自动恢复**。近一步，如果整个系统由多个相关联的子系统组成，某子系统的故障通过耦合关系引起其他子系统发生故障，最终导致整个系统可用性严重下降甚至完全不可用，我们称这种现象为**相继故障**(也称作级联故障，英文名cascading failure), 也就是常说的**雪崩**。

#### 过载案例分析  
下面通过一个具体案例来分析下过载现象。  
##### 基本情况:  
如下图所示， 模块A是一个使用[Reactor网络编程模式](http://gao-xiao-long.github.io/2017/05/21/network-io/)的纯转发系统，采用多线程并行的将用户的请求转发到模块B，并同步得到模块B的返回结果，返回给用户。
![图](/img/in-post/over_flow1.png)
上图中展现了的模块A大致的内部结构，其中
* 内核为每个连接都建立了一个Recv-Q和Send-Q
* Reactor负责与client建立连接、读取Recv-Q中的数据、发送响应数据到Send-Q
* thread pool负责对读取数据的解码、处理、编码等。

**对于单个请求，它的处理逻辑可以如下描述:**  
　　Step1： 从Socket缓冲区(Recv-Q)接受用户请求并解析  
　　Step2： 进行本地逻辑处理
　　Step3： 发送请求到后端模块B  
　　Step4： 同步等待后端模块B返回  
　　Step5： 接收后端模块B的应答  
　　Step6： 应答前端用户  

**正常情况下，假设：**
* 用户请求模块A的报文大小100Bytes，为简单起见，只有一个用户请求模块A，采用长连接形式，请求峰值QPS为800
* 模块A采用10个线程并行处理请求，每个连接设置的接受缓冲区(Recv-Q)大小为:229376Bytes(此值为某线上机器的默认值)
* 模块A在处理请求时，做纯转发操作，本地运算耗时非常少，可以忽略不计
* 后端模块B并行能力很高，可以处理的极限QPS为10000次以上，且请求处理延迟不超过10ms
* 上游对模块A定义的读时间为2s，模块A对模块B定义的读超时时间为1s

**根据前面的假设，我们可以得到以下数据:**
* 模块A在正常情况下可以处理的极限QPS为:1000。计算方法：单线程每秒钟可以处理1000(1s) / 10 = 100个请求，10个线程并行处理则可以处理10 * 100 = 1000
* 模块A的Recv-Q平均可以缓存的请求数为:22937个。计算方法：Recv-Q大小 / 每个请求包大小

##### 过载分析:  
**导火索：**
某天模块B进行了新特性发布，每个请求**处理延时从10ms增长到40ms**，这时，随着时间推移，发现所有经过模块A的请求都超时，
模块A的对外处理能力变为0。

**过载分析：**  
正常情况下模块A最大处理QPS为1000， 而用户的请求峰值QPS是800，模块A足以将用户每秒800峰值处理完成。当模块B的每个请求**处理延时从10ms增长到40ms**时，这时候模块A的最大处理QPS变成了(1000 / 40) * 10 = 250。远小于800qps。因为请求量和处理能力的差距，每秒钟有550个(800-250)请求无法及时处理，被缓存到Recv-Q，并且使得缓冲区在4s内被填满(每秒550个积压请求，每个请求100占100字节，缓冲区一共229376字节，229376 / (550*100) = 4s)。在压力不变的情况下，模块A的缓冲区将一直保持满的状态。 这意味着，一个请求被追加到缓冲区里后，要等待91秒(缓存22937个请求，每秒处理250个，需要91秒)才能被模块A取出来处理，这时候用户早就已经超时了，也就是说，进程A每次处理的请求，都已经是91s以前产生的，**模块A一直在做无用功。对外处理能力表现为0**。下图可以比较直观的展现Recv-Q中请求被处理的延迟
![图](/img/in-post/recv_queue.png)

* ”最短等待处理时间“指的是请求从"发送到模块A"到"被模块A开始处理"时等待的最短时间，比如：第1-250条请求最短等待时间为0，如果请求是同一时刻发送过来的，那么理论上前10条请求等待时间为0s(10个线程同时处理)，第10到20个请求等待处理时间为20ms(j每个线程处理耗时为20ms，处理完后再取新的请求)，20到20个请求等待处理时间为40ms，以此类推。
* 缓冲区被填满的情况 可以用水库做比较，如果进入水库的如水量大于水库闸门放水的出水量，随着时间的推移，水库一定会被填满。

为了更直观的观察过载发生时的情况，写了一个小程序来模拟。
1. echo_server为服务器，极限QPS大概在60左右，每个请求响应时间为100ms。
2. echo_client为客户端，通过长连接方式访问echo_server，设置的读超时时间为300ms，网络耗损在1ms左右。
程序通过自动增加echo_client的QPS，直到超出echo_server的极限，来模拟过载时发生的状况。

下图是echo_client打印的请求日志: every_5s_cuss指的是最近5秒内请求成功的个数，every_5s_fail指的是最近5秒内请求失败的个数，every_5s_latency_avg指的是最近5秒的平均响应时间。
![图](/img/in-post/client_1.png)

从图中可以看出，当请求的QPS从50调整到98之后，超出echo_server的极限性能，这时候在QPS基本不变的情况下every_5s_fail明显增多，从0直接跳变到300以上。

下面是保持相同的QPS几秒钟之后，基本上所有的请求都失败，**系统展现的可用性为0**
![图](/img/in-post/client_2.png)

同时，我们看到echo_server端的Recv-Q中有大量数据积压:
![图](/img/in-post/netstat_full.png)

下图是系统的对外表现与发送QPS的关系图：
![图](/img/in-post/overload_display.png)

PS: 上面为了方便分析，只列出了单连接的情况，多个长连接及短连接情况类似，只要请求量超过服务的处理量，并且这时候没有有效的处理措施，随着时间的积压，就一定会导致服务可用性急剧下降。

**过载根本原因：**

除了上面讲的因下游模块升级导致上游过载外，还有其他可能引起过载
* 下游模块B大规模故障：类似于模块A访问模块B的延迟由10ms变成了“超时时间”
* 上游请求模块A的流量剧增：比如因为cache击穿或者秒杀活动等导致流量剧增
* 同机其他模块占用过多CPU或者网卡资源，导致模块A的处理性能降低

以上所有原因都可以归结为两点：
1. 一是处理能力的下降。
2. 二是请求量的上升。

#### Proxy层如何预防过载引起的雪崩

Proxy层设计预防过载及雪崩的目标是：在系统过载时，服务还能提供一个稳定的较高的处理能力。即，如下图所示，在系统过载时系统还能保持“处理成功QPS”的稳定性。
[图](/img/in-post/overload_perfect.png)

主要思路是从三个方面入手
1. 对Proxy模块进行保护，在Proxy层出现过载时，避免出现整体不可用。
2. 对Proxy的下游模块进行保护，尽量避免下游出现过载。
3. 当下游模块出现过载时，能保证下游及时恢复。
之所以要避免下游“出现过载”是由于下游模块的复杂性，我们不能保证所有的下游模块都具备过载保护功能，所以在Proxy要做的就是控制往下游发送的请求数，尽量避免下游出现过载的情况。
当下游出现过载时，我们也需要想办法让其尽快恢复正常。

下面一一阐述上面三个方面的具体办法
1. 如何对Proxy模块进行保护，在Proxy层出现过载时，避免出现整体不可用？

（1） 资源隔离
Proxy由多个下游组成，有可能出现某个下游模块因为升级导致平响升高，或者某个Bug引发服务不可用的情况。这样会间接影响导致Proxy整体的转发性能降低，出现过载，进而影响对整个下游的转发。
对应的解决思路就是“资源隔离”，资源隔离主要有两种方法，一种是线程等层面的隔离，一种是部署层面的隔离。
线程层面隔离如下图:
[图](/img/in-post/geli_2.png)
在线程层面，为下游模块分配好资源，比如Proxy一共开启了100个处理线程。我们规定下游模块A只能使用30个线程，模块B和模块C分别只能使用20个线程。当模块A使用的线程数达到阈值后，主动拒绝新来的请求。
这样，即使模块A出现故障，也不会影响整个Proxy。但是这种方案有个缺点：很多系统都是使用的通用的网络框架，如grpc, sofa-pbrpc，这种线程隔离的思路很难在网络框架层之上实现，即使实现了也很难做到高效转发(各种锁、资源争夺等)
部署层面的隔离如下图：
[图](/img/in-post/geli_3.png)
这种隔离的主要思路是轻重分离，为某些流量大或者对系统SLA要求比较高的服务单独部署Proxy，做到物理层的隔离。跟线程层面隔离相比，这种隔离比较简单，不需要改动代码，缺点是上游还需要根据不同的请求分发到不同的Proxy中。
但是在实际生产环境中，部署层面隔离是一个比较好的选择。

(2) 防止因自身转发性能不足引发的过载
实际生产环境下存在一种可能就是下游服务可以承担的很大的并发，但是Proxy本身由于实例数等各种原因导致自身成为转发瓶颈，引发过载，可能的解决思路有以下几种
1. 检测请求从"发送出去"到"开始被处理“之间的时延，超过一定阈值直接丢弃。
此方法主要的思想就是避免处理无用的请求，假设上游向Proxy发送请求，定义的超时时间为2s，Proxy在处理某个请求时发现它是两秒之前发送过来的，则可以直接丢弃，防止Recv-Q中消息的堆积。
具体的实现上有两种方式：
(1) 协议层加上时间戳，即每个消息都带有发送时间。Proxy在处理时根据过期时间选择是否丢弃。此种实现方式比较简单，但是最大的问题在于可能存在“两台机器时钟时间不一致”情况，比如发送方的机器时钟比Proxy的机器时钟本身就慢2秒。这时候即使是新发送到Proxy的请求，也会被Proxy认为是2秒前发送的，有可能直接丢弃。
(2) 使用socket选项SO_TIMESTAMP，通过带外数据获取到数据到达系统缓冲区的时间。
    这种实现思路虽然也有本机时钟时间修改问题，但是每次修改只影响缓冲区中的数据，所以影响不大。问题是它框架层(比如grpc等)实现支持支持，但是目前最主要问题是要Server框架层基本上不支持此功能。
2. 为消息建立任务队列，当Socket中有可读消息时，将消息读到任务队列中，然后分发给工作线程处理。任务队列的大小根据实际的处理性能决定，如果任务队列已满，则直接丢弃该消息。
    此类实现方式也是最好在框架层支持，避免在应用层实现影响转发效率(各种资源、锁的争夺等)


2. 如何对Proxy的下游模块进行保护，尽量避免下游出现过载。
（1）选择好的负载均衡策略，将请求流量更多的分担到性能更好的机器，并在单机出现过载时，能很快的将其压力分担到其他实例中。防止单机压力过大（讲下LA算法）
（2）合理配置重试：重试默认只在连接出错时发起，防止系统读写超时时频繁重试导致流量加剧。
（3）前端防御：Proxy根据后端负载能力设置一个最大并发值，超过最大并发直接拒绝。通常通过 QPS*Latency 设置最大并发
  设置方法有两种，一种是全局设置，即全局范围内设置一个最大并发值，当
  另一种方式是局部设置，根据全局最大并发分摊到某台实例上。 优缺点？ 秒级别流量控制粒度太细？上游负载均衡策略影响？

当过载发生时，该拒绝的请求(1. 超出整个系统处理能力范围的；2.已经超时的无效请求)越早拒绝越好。
监测下游的响应时间状况，当出现不稳定时，控制下发速率，比如先将线程sleep(多少时间)再择机恢复。
中间层server对后端方请求，重试机制要慎用，一定要用的话要有严格频率控制。
过载保护很重要的一点，不是说要加强系统性能、容量、成功应答所有请求，而是保证高压下，系统的服务能力不要徒降到0，而是顽强的对外展现最大有效处理能力。
各种系统资源监控即及时报警机制（不动运维的RD不是好RD）


3. 如何保证当下游模块出现过载时，能保证下游及时恢复。
过载恢复的条件：1. 满足两个条件：请求量低于处理能力(腾讯文章2)
        2. 所有连接都不处于丢包状态(腾讯文章2)
思路：（1） 人工触发降级预案，丢弃多少流量，巴拉巴拉
      (2) 动态调节(阿里技术家年华的文章：类似于断路器， 正常状态、过载保护、灰度恢复)，过载保护状态可以（1）控制下发速率 （2）控制下发比例。总之是降低QPS,动态调节相当于服务降级，排空请求队列(排空的时间取决于积压的请求数 / (最大qps - 当前qps))
如何才能够确保这个灰度的值呢？应该是根据这个排空请求队列公式来算。






附：基础知识点：Recv-Q及Send-Q
1. 概念
内核为每个TCP连接在内核中都创建了一个发送缓冲区和一个接收缓冲区，TCP的全双工的工作模式以及TCP的流量(拥塞)控制便是依赖于这两个独立的buffer以及buffer的填充状态。
接收缓冲区把数据缓存入内核，应用进程一直没有调用recv()进行读取的话，此数据会一直缓存在相应socket的接收缓冲区内。
再啰嗦一点，不管进程是否调用recv()读取socket，对端发来的数据都会经由内核接收并且缓存到socket的内核接收缓冲区之中。
recv()所做的工作，就是把内核缓冲区中的数据拷贝到应用层用户的buffer里面，并返回，仅此而已。进程调用send()发送的数据的时候，最简单情况（也是一般情况），将数据拷贝进入socket的内核发送缓冲区之中，然
后send便会在上层返回。换句话说，send（）返回之时，数据不一定会发送到对端去（和write写文件有点类似），
send()仅仅是把应用层buffer的数据拷贝进socket的内核发送buffer中，发送是TCP的事情，和send其实没有太大关系。接收缓冲区被TCP用来缓存网络上来的数据，
一直保存到应用进程读走为止。对于TCP，如果应用进程一直没有读取，接收缓冲区满了之后，发生的动作是：收端通知发端，接收窗口关闭（win=0）。这个便是滑动窗口的实现。
保证TCP套接口接收缓冲区不会溢出，从而保证了TCP是可靠传输。因为对方不允许发出超过所通告窗口大小的数据。 这就是TCP的流量控制，如果对方无视窗口大小而发出了超过窗口大小的数据，则接收方TCP将丢弃它。
2. 如何修改及定义  内核/程序
表 1. TCP/IP 栈性能使用的可调节内核参数
可调节的参数	默认值	选项说明   （并不是说可以动态变化的。指的是默认值是多少，最大值是多少）
/proc/sys/net/core/rmem_default	"110592"	定义默认的接收窗口大小；对于更大的 BDP 来说，这个大小也应该更大。
/proc/sys/net/core/rmem_max	"110592"	定义接收窗口的最大大小；对于更大的 BDP 来说，这个大小也应该更大。
/proc/sys/net/core/wmem_default	"110592"	定义默认的发送窗口大小；对于更大的 BDP 来说，这个大小也应该更大。
/proc/sys/net/core/wmem_max	"110592"	定义发送窗口的最大大小；对于更大的 BDP 来说，这个大小也应该更大。
3. 实际线上环境如何查看Recv-Q和Send-Q状态

2. 线上压测的一些误区： 通过服务端的日志条数及耗时来给出服务的极限QPS，这有很大的漏洞及隐患。
参考：
1. 阿里技术嘉年华046-海量服务之过载保护
2. 服务器过载保护（上篇）——过载介绍(http://wetest.qq.com/lab/view/69.html)
3. 服务器过载保护（下篇）——过载处理新方案(http://wetest.qq.com/lab/view/70.html)
6. Cache应用中的服务过载案例研究(http://tech.meituan.com/avalanche-study.html）
7. Linux Tune Network Stack (Buffers Size) To Increase Networking Performance(https://www.cyberciti.biz/faq/linux-tcp-tuning/)

http://blog.csdn.net/Joanna_yan/article/details/51226211
http://blog.csdn.net/Joanna_yan/article/details/51226211
